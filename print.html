<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Notes</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="microprocessors/index.html"><strong aria-hidden="true">1.</strong> Microprocessors</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="microprocessors/history.html"><strong aria-hidden="true">1.1.</strong> History</a></li><li class="chapter-item expanded "><a href="microprocessors/index.html"><strong aria-hidden="true">1.2.</strong> 8086 Microprocessor</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="microprocessors/architecture.html"><strong aria-hidden="true">1.2.1.</strong> Architecture</a></li><li class="chapter-item expanded "><a href="microprocessors/addressing-modes.html"><strong aria-hidden="true">1.2.2.</strong> Addressing Modes</a></li><li class="chapter-item expanded "><a href="microprocessors/registors-and-flags.html"><strong aria-hidden="true">1.2.3.</strong> Registers and Flags</a></li><li class="chapter-item expanded "><a href="microprocessors/memory-segmentation.html"><strong aria-hidden="true">1.2.4.</strong> Memory Segmentation</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="computer-architecture/index.html"><strong aria-hidden="true">2.</strong> Computer Architecture</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="computer-architecture/1.intro.html"><strong aria-hidden="true">2.1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="computer-architecture/2.performance-measurement.html"><strong aria-hidden="true">2.2.</strong> Performance Measurement</a></li><li class="chapter-item expanded "><a href="computer-architecture/3.intro-to-risc.html"><strong aria-hidden="true">2.3.</strong> Intro to RISC</a></li><li class="chapter-item expanded "><a href="computer-architecture/4.pipeline-hazards.html"><strong aria-hidden="true">2.4.</strong> Pipeline Hazards</a></li><li class="chapter-item expanded "><a href="computer-architecture/6.compiler-techniques.html"><strong aria-hidden="true">2.5.</strong> Compiler Techniques</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Notes</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="microprocessors"><a class="header" href="#microprocessors">Microprocessors</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-the-microprocessor-and-computer"><a class="header" href="#introduction-to-the-microprocessor-and-computer">Introduction to the Microprocessor and Computer</a></h1>
<p>The world’s first microprocessor, the Intel 4004, was a 4-bit
microprocessor–programmable controller on a chip. It addressed a mere 4096,
4-bit-wide memory locations. The 4004 instruction set contained only 45
instructions. It was fabricated with the then-current state-of-the-art
P-channel MOSFET technology that only allowed it to execute instructions at the
slow rate of 50 KIPs (kilo-instructions per second).
This was slow when compared to the 100,000 instructions executed per second by
the 30-ton ENIAC computer in 1946. The main difference was that the 4004
weighed much less than an ounce. The main problems with this early microprocessor
were its speed, word width, and memory size.</p>
<h2 id="the-8085-microprocessor"><a class="header" href="#the-8085-microprocessor">The 8085 Microprocessor</a></h2>
<p>In 1977, Intel Corporation introduced an updated version of the 8080—the 8085.
The 8085 was to be the last 8-bit, general-purpose microprocessor developed by
Intel. Although only slightly more advanced than an 8080 microprocessor, the
8085 executed software at an even higher speed. An addition that took 2.0 μs
(500,000 instructions per second on the 8080) required only 1.3 μs (769,230
instructions per second) on the 8085. The main advantages of the 8085 were its
internal clock generator, internal system controller, and higher clock
frequency. This higher level of component integration reduced the 8085’s cost
and increased its usefulness. </p>
<p>The 16-bit microprocessor evolved mainly because of the need for larger memory systems.
The popularity of the Intel family was ensured in 1981, when IBM Corporation decided to use
the 8088 microprocessor in its personal computer. The 16-bit 8086 and 8088 provided 1M byte of memory.</p>
<h2 id="the-80286-microprocessor"><a class="header" href="#the-80286-microprocessor">The 80286 Microprocessor</a></h2>
<p>The 80286 microprocessor (also a 16-bit architecture microprocessor)
was almost identical to the 8086 and 8088, except it addressed a 16M-byte memory system instead
of a 1M-byte system. The instruction set of the 80286 was almost identical to the 8086 and 8088,
except for a few additional instructions that managed the extra 15M bytes of memory. The clock
speed of the 80286 was increased, so it executed some instructions in as little as 250 ns (4.0 MIPs)
with the original release 8.0 MHz version. Some changes also occurred to the internal execution of
the instructions, which led to an eightfold increase in speed for many instructions when compared to
8086/8088 instructions.</p>
<h2 id="the-32-bit-microprocessor"><a class="header" href="#the-32-bit-microprocessor">The 32-Bit Microprocessor</a></h2>
<p>Applications began to demand faster microprocessor speeds, more
memory, and wider data paths. This led to the arrival of the 80386 in 1986 by Intel Corporation.
The 80386 represented a major overhaul of the 16-bit 8086–80286 architecture. The 80386 was
Intel’s first practical 32-bit microprocessor that contained a 32-bit data bus and a 32-bit memory
address.</p>
<p>The 80386 was available in a few modified versions such as the 80386SX, which
addressed 16M bytes of memory through a 16-bit data and 24-bit address bus, and
the 80386SL/80386SLC, which addressed 32M bytes of memory through a 16-bit data
and 25-bit address bus. An 80386SLC version contained an internal cache memory
that allowed it to process data at even higher rates. In 1995, Intel released
the 80386EX microprocessor. The 80386EX microprocessor is called an embedded PC
because it contains all the components of the AT class personal computer on a
single integrated circuit. The 80386EX also contains 24 lines for input/output
data, a 26-bit address bus, a 16-bit data bus, a DRAM refresh controller, and
programmable chip selection logic.</p>
<h2 id="the-80486-microprocessor"><a class="header" href="#the-80486-microprocessor">The 80486 Microprocessor.</a></h2>
<p>In 1989, Intel released the 80486 microprocessor, which incorpo- rated an
80386-like microprocessor, an 80387-like numeric coprocessor, and an 8K-byte
cache memory system into one integrated package. Although the 80486
microprocessor was not radi- cally different from the 80386, it did include one
substantial change. The internal structure of the 80486 was modified from the
80386 so that about half of its instructions executed in one clock instead of
two clocks. Because the 80486 was available in a 50 MHz version, about half of
the instructions executed in 25 ns (50 MIPs). The average speed improvement for
a typical mix of instructions was about 50% over the 80386 that operated at the
same clock speed. Later versions of the 80486 executed instructions at even
higher speeds with a 66 MHz double-clocked version (80486DX2). The
double-clocked 66 MHz version executed instructions at the rate of 66 MHz, with
memory transfers executing at the rate of 33 MHz. (This is why it was called a
double-clocked microprocessor.) A triple-clocked version from Intel, the
80486DX4, improved the internal execution speed to 100 MHz with memory
transfers at 33 MHz. Note that the 80486DX4 microprocessor executed
instructions at about the same speed as the 60 MHz Pentium. It also contained
an expanded 16K-byte cache in place of the standard 8K-byte cache found on
earlier 80486 microprocessors. Advanced Micro Devices (AMD) has produced a
triple-clocked version that runs with a bus speed of 40 MHz and a clock speed
of 120 MHz. The future promises to bring microprocessors that internally
execute instructions at rates of up to 10 GHz or higher.</p>
<h2 id="pentium-ii-and-pentium-xeon-microprocessors"><a class="header" href="#pentium-ii-and-pentium-xeon-microprocessors">Pentium II and Pentium Xeon Microprocessors</a></h2>
<p>The Pentium II microprocessor (released in 1997) represents a new direction for
Intel. Instead of being an integrated circuit as with prior ver- sions of the
microprocessor, Intel has placed the Pentium II on a small circuit board. The
main reason for the change is that the L2 cache found on the main circuit board
of the Pentium was not fast enough to function properly with the Pentium II. On
the Pentium system, the L2 cache oper- ates at the system bus speed of 60 MHz
or 66 MHz. The L2 cache and microprocessor are on a circuit board called the
Pentium II module. This onboard L2 cache operates at a speed of 133 MHz and
stores 512K bytes of information. The microprocessor on the Pentium II module
is actually Pentium Pro with MMX extensions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="microprocessors-1"><a class="header" href="#microprocessors-1">Microprocessors</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>The addressing modes for this powerful family of microprocessors are described
for the real, protected, and flat modes of operation. Real mode memory (DOS
memory) exists at locations 00000H–FFFFFH, the first 1M byte of the memory
system, and is present on all versions of the microprocessor. Protected mode
memory (Windows memory) exists at any location in the entire protected memory
system, but is available only to the 80286–Core2, not to the earlier 8086 or
8088 microprocessors. Protected mode memory for the 80286 contains 16M bytes;
for the 80386–Pentium, 4G bytes; and for the Pentium Pro through the Core2,
either 4G or 64G bytes. With the 64-bit extensions enabled, the Pentium 4
and Core2 address 1T byte of memory in a flat memory model. Windows Vista
or Windows 64 is needed to operate the Pentium 4 or Core2 in 64-bit mode
using the flat mode memory to access the entire 1T byte of memory.</p>
<h1 id="8086"><a class="header" href="#8086">8086</a></h1>
<p><img src="microprocessors/./images/8086_architecture-diagram.jpg" alt="architecture_diagram" /></p>
<ul>
<li>8086 does not have a RAM or ROM inside it. However, it has internal registers for storing intermediate and final results and interfaces with memory located outside it through the System Bus.</li>
<li>It is a 16-bit Integer processor in a 40 pin, Dual Inline Packaged IC.</li>
<li>The size of the internal registers(present within the chip) indicate how much information the processor can operate on at a time (in this case 16-bit registers) and how it moves data around internally within the chip, sometimes also referred to as the internal data bus.</li>
<li>8086 provides the programmer with 14 internal registers, each 16 bits or 2 Bytes wide.</li>
</ul>
<h3 id="memory-segmentation"><a class="header" href="#memory-segmentation">Memory segmentation:</a></h3>
<ul>
<li>To increase execution speed and fetching speed, 8086 segments the memory. </li>
<li>It’s 20 bit address bus can address 1MB of memory, it segments it into 16 64kB segments. </li>
<li>8086 works only with four 64KB segments within the whole 1MB memory.</li>
</ul>
<h3 id="the-internal-architecture-is-divided-into-2-units"><a class="header" href="#the-internal-architecture-is-divided-into-2-units">The internal architecture is divided into 2 units</a></h3>
<ol>
<li>The Bus Interface Unit (BIU)</li>
<li>The Execution Unit (EU)</li>
</ol>
<h2 id="1-the-bus-interface-unit-biu"><a class="header" href="#1-the-bus-interface-unit-biu">1. The Bus Interface Unit (BIU)</a></h2>
<ul>
<li>It provides the interface of 8086 to external memory and I/O devices via the System Bus. It performs various machine cycles such as memory read, I/O read etc. to transfer data between memory and I/O devices. </li>
</ul>
<h3 id="role-of-biu"><a class="header" href="#role-of-biu">Role of BIU</a></h3>
<ul>
<li>It generates the 20 bit physical address for memory access.</li>
<li>It fetches instructions from the memory.</li>
<li>It transfers data to and from the memory and I/O.</li>
<li>Maintains the 6 byte prefetch instruction queue(supports pipelining).</li>
<li>BIU mainly contains the 4 Segment registers, the Instruction Pointer, a prefetch queue and an Address Generation Circuit</li>
</ul>
<h3 id="instruction-pointer-ip"><a class="header" href="#instruction-pointer-ip">Instruction Pointer (IP):</a></h3>
<ul>
<li>It is a 16 bit register. It holds offset of the next instructions in the <strong>Code Segment</strong></li>
<li>IP is incremented after every instruction byte is fetched.</li>
<li>IP gets a new value whenever a branch instruction occurs.</li>
<li>CS is multiplied by 10H to give the 20 bit physical address of the Code Segment.</li>
<li>Address of the next instruction is calculated as <strong>CS x 10H + IP</strong>.</li>
</ul>
<p>Example: </p>
<pre><code>CS = 4321H 
IP = 1000H 
then CS x 10H = 43210H + offset =  44210H  
</code></pre>
<h3 id="code-segment-register"><a class="header" href="#code-segment-register">Code Segment register</a></h3>
<ul>
<li>CS holds the base address for the Code Segment. </li>
<li>All programs are stored in the Code Segment and accessed via the IP(instruction pointer register). </li>
<li>CS register cannot be modified by executing any instruction except branch instructions</li>
</ul>
<h3 id="data-segment-register"><a class="header" href="#data-segment-register">Data Segment register</a></h3>
<p>DS holds the base address for the Data Segment. </p>
<h3 id="stack-segment-register"><a class="header" href="#stack-segment-register">Stack Segment register</a></h3>
<p>SS holds the base address for the Stack Segment. </p>
<h3 id="extra-segment-register"><a class="header" href="#extra-segment-register">Extra Segment register</a></h3>
<p>ES holds the base address for the Extra Segment. </p>
<h3 id="address-generation-circuit"><a class="header" href="#address-generation-circuit">Address Generation Circuit:</a></h3>
<ul>
<li>The BIU has a Physical Address Generation Circuit.</li>
<li>It generates the 20 bit physical address using Segment and Offset addresses using the formula: </li>
</ul>
<pre><code>Physical Address = Segment Address x 10H + Offset Address
</code></pre>
<h3 id="6-byte-pre-fetch-queue"><a class="header" href="#6-byte-pre-fetch-queue">6 Byte Pre-fetch Queue</a></h3>
<ul>
<li>It is a 6-byte FIFO RAM used to implement Pipelining.</li>
<li>Fetching the next instruction (by BIU from CS) while executing the current instruction is called <strong>pipelining</strong>.</li>
<li>BIU fetches the next &quot;six instruction-bytes&quot; from the Code Segment and stores it into the queue. Execution Unit (EU) removes instructions from the queue and executes them.</li>
<li>The queue is refilled when atleast two bytes are empty as 8086 has a 16-bit data bus.</li>
<li>Pipelining fails when a branch occurs, as the pre-fetched instructions are no longer useful. Hence as soon as 8086 detects a branch operation, it clears/discards the entire queue. Now, the next six bytes from the new location (branch address) are fetched and stored in the queue and Pipelining continues.</li>
</ul>
<h2 id="2-the-execution-unit-eu"><a class="header" href="#2-the-execution-unit-eu">2. The Execution Unit (EU)</a></h2>
<p>The main components of the EU are General purpose registers, the ALU, Special purpose registers, Instruction Register and Instruction Decoder and the Flag/Status Register. </p>
<ul>
<li>Fetches instructions from the Queue in BIU, decodes and executes arithmetic and logic operations using the ALU.</li>
<li>Sends control signals for internal data transfer operations within the microprocessor.</li>
<li>Sends request signals to the BIU to access the external module.</li>
<li>It operates with respect to clock cycles (T-states) and not machine cycles.</li>
<li>8086 has four 16 bit general purpose registers AX, BX, CX and DX. Store intermediate values during execution. Each of these have two 8 bit parts (higher and lower). </li>
</ul>
<h4 id="ax-register"><a class="header" href="#ax-register">AX register</a></h4>
<p>It holds operands and results during multiplication and division operations. Also an accumulator during String operations. </p>
<h4 id="bx-register"><a class="header" href="#bx-register">BX register</a></h4>
<p>It holds the memory address (offset address) in indirect addressing modes. </p>
<h4 id="cx-register"><a class="header" href="#cx-register">CX register</a></h4>
<p>It holds count for instructions like loop, rotate, shift and string operations. </p>
<h4 id="dx-register"><a class="header" href="#dx-register">DX register</a></h4>
<p>It is used with AX to hold 32 bit values during multiplication and division. </p>
<h3 id="arithmetic-logic-unit-16-bit"><a class="header" href="#arithmetic-logic-unit-16-bit">Arithmetic Logic Unit (16 bit)</a></h3>
<p>Performs 8 and 16 bit arithmetic and logic operations. </p>
<h3 id="special-purpose-registers-16-bit"><a class="header" href="#special-purpose-registers-16-bit">Special purpose registers (16-bit)</a></h3>
<ol>
<li><strong>Stack Pointer(SP)</strong></li>
</ol>
<ul>
<li>Points to Stack top. </li>
<li>Stack is in Stack Segment, used during instructions like PUSH, POP, CALL, RET etc.</li>
</ul>
<ol start="2">
<li><strong>Base Pointer(BP)</strong></li>
</ol>
<ul>
<li>BP can hold offset address of any location in the stack segment. </li>
<li>It is used to access random locations of the stack.</li>
</ul>
<ol start="3">
<li><strong>Source Index(SI)</strong></li>
</ol>
<ul>
<li>It holds offset address in Data Segment during string operations.</li>
</ul>
<ol start="4">
<li><strong>Destination Index(DI)</strong></li>
</ol>
<ul>
<li>It holds offset address in Extra Segment during string operations.</li>
</ul>
<h3 id="instruction-register-and-instruction-decoder"><a class="header" href="#instruction-register-and-instruction-decoder">Instruction Register and Instruction Decoder</a></h3>
<p>The EU fetches an opcode from the queue into the instruction register. The instruction decoder decodes it and sends the information to the control circuit for execution. </p>
<h3 id="flagstatus-register-16-bits"><a class="header" href="#flagstatus-register-16-bits">Flag/Status register (16 bits):</a></h3>
<p>It has 9 flags that help change or recognize the state of the microprocessor. </p>
<p><img src="microprocessors/./images/8086_flags.png" alt="flags" /> </p>
<h3 id="6-status-flags"><a class="header" href="#6-status-flags">6 Status flags:</a></h3>
<ol>
<li>
<p>carry flag(CF)
It is set whenever there is a carry {or borrow} out of the MSB of a the result (D7 bit for an 8-bit operation D15 bit for a 16-bit operation)</p>
</li>
<li>
<p>parity flag(PF)
It is set if the result has even parity.</p>
</li>
<li>
<p>auxiliary carry flag(AF)
It is set if a carry is generated out of the Lower Nibble.
It is used only in 8-bit operations like DAA and DAS.</p>
</li>
<li>
<p>zero flag(Z)
It is set if the result is zero.</p>
</li>
<li>
<p>sign flag(S)
It is set if the MSB of the result is 1.
For signed operations, such a number is treated as –ve.</p>
</li>
<li>
<p>overflow flag (O)
It will be set if the result of a signed operation is too large to fit in the number of bits available to represent it. It can be checked using the instruction INTO (Interrupt on Overflow).</p>
</li>
</ol>
<p>Status flags are updated after every arithmetic and logic operation. </p>
<h3 id="3-control-flags"><a class="header" href="#3-control-flags">3 Control flags:</a></h3>
<ol>
<li>
<p>trap flag(TF)
It is used to set the Trace Mode i.e. start Single Stepping Mode.
Here the µP is interrupted after every instruction so that, the program can be debugged.</p>
</li>
<li>
<p>interrupt flag(IF)
It is used to mask (disable) or unmask (enable) the INTR interrupt.</p>
</li>
<li>
<p>direction flag(DF)
If this flag is set, SI and DI are in auto-decrementing mode in String Operations.</p>
</li>
</ol>
<p>These flags can be set or reset using control instructions like CLC, STC, CLD, STD, CLI, STI, etc</p>
<p>The Control flags are used to control certain operations. </p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li>https://lambdageeks.com/8086-microprocessors-pin-diagram/</li>
<li>https://www.geeksforgeeks.org/architecture-of-8086/</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="general-purpose-registers-in-8086-microprocessor"><a class="header" href="#general-purpose-registers-in-8086-microprocessor">General purpose registers in 8086 microprocessor</a></h1>
<p>General purpose registers are used to store temporary data within the microprocessor. There are 8 general purpose registers in 8086 microprocessor.</p>
<p><img src="microprocessors/./images/memory-segment.png" alt="memory-segment" /> </p>
<p>AX, BX, CX, DX are of 16 bits and is divided into two 8-bit registers XH and XL to also perform 8-bit instructions.
SP, BP, SI, DI are of 16 bits and cannot be devided.</p>
<ol>
<li>AX </li>
</ol>
<ul>
<li>This is the accumulator. </li>
<li>It is generally used for arithmetical and logical instructions but in 8086 microprocessor it is not mandatory to have accumulator as the destination operand.
Example:</li>
</ul>
<pre><code>ADD AX, AX  //(AX = AX + AX)
</code></pre>
<ol start="2">
<li>BX </li>
</ol>
<ul>
<li>This is the base register. </li>
<li>It is used to store the value of the offset.
Example:</li>
</ul>
<pre><code>MOV BL, [500] (BL = 500H)
</code></pre>
<ol start="3">
<li>CX </li>
</ol>
<ul>
<li>This is the counter register. </li>
<li>It is used in looping and rotation.
Example:</li>
</ul>
<pre><code>MOV CX, 0005
LOOP
</code></pre>
<ol start="4">
<li>DX </li>
</ol>
<ul>
<li>This is the data register. </li>
<li>It is used in multiplication an input/output port addressing.
Example:</li>
</ul>
<pre><code>MUL BX (DX, AX = AX * BX)
</code></pre>
<ol start="5">
<li>SP (stack pointer)</li>
</ol>
<ul>
<li>It points to the topmost item of the stack.</li>
<li>If the stack is empty the stack pointer will be (FFFE)H.</li>
<li>Stack is in Stack Segment, used during instructions like PUSH, POP, CALL, RET etc.</li>
<li>It’s offset address relative to stack segment.</li>
</ul>
<ol start="6">
<li>BP (base pointer)</li>
</ol>
<ul>
<li>It is primary used in accessing parameters passed by the stack.</li>
<li>It’s offset address relative to stack segment.</li>
</ul>
<ol start="7">
<li>SI (source index register)</li>
</ol>
<ul>
<li>It is used in the pointer addressing of data and as a source in some string related operations.</li>
<li>It’s offset is relative to data segment.</li>
</ul>
<ol start="8">
<li>DI (destination index register)</li>
</ol>
<ul>
<li>It is used in the pointer addressing of data and as a destination in some string related operations.</li>
<li>It’s offset is relative to extra segment.</li>
</ul>
<h4 id=""><a class="header" href="#"></a></h4>
<h3 id="1-sign-flag-s"><a class="header" href="#1-sign-flag-s">1. Sign Flag (S)</a></h3>
<ul>
<li>After any operation if the MSB (B(7)) of the result is 1, it indicates the number is negative and the sign flag becomes set, i.e. 1. If the MSB is 0, it indicates the number is positive and the sign flag becomes reset i.e. 0.</li>
<li>from 00H to 7F, sign flag is 0</li>
<li>from 80H to FF, sign flag is 1</li>
<li>1- MSB is 1 (negative)</li>
<li>0- MSB is 0 (positive)</li>
</ul>
<p>Example:</p>
<p>MVI A 30 (load 30H in register A)
MVI B 40 (load 40H in register B)
SUB B (A = A – B)
These set of instructions will set the sign flag to 1 as 30 – 40 is a negative number.</p>
<p>MVI A 40 (load 40H in register A)
MVI B 30 (load 30H in register B)
SUB B (A = A – B)
These set of instructions will reset the sign flag to 0 as 40 – 30 is a positive number.</p>
<p>Zero Flag (Z) – After any arithmetical or logical operation if the result is 0 (00)H, the zero flag becomes set i.e. 1, otherwise it becomes reset i.e. 0.
00H zero flag is 1.
from 01H to FFH zero flag is 0
1- zero result
0- non-zero result</p>
<p>Example:</p>
<p>MVI A 10 (load 10H in register A)
SUB A (A = A – A)
These set of instructions will set the zero flag to 1 as 10H – 10H is 00H</p>
<p>Auxiliary Carry Flag (AC) – This flag is used in BCD number system(0-9). If after any arithmetic or logical operation D(3) generates any carry and passes on to B(4) this flag becomes set i.e. 1, otherwise it becomes reset i.e. 0. This is the only flag register which is not accessible by the programmer
1-carry out from bit 3 on addition or borrow into bit 3 on subtraction
0-otherwise</p>
<p>Example:</p>
<p>MOV A 2B (load 2BH in register A)
MOV B 39 (load 39H in register B)
ADD B (A = A + B)
These set of instructions will set the auxiliary carry flag to 1, as on adding 2B and 39, addition of lower order nibbles B and 9 will generate a carry.</p>
<p>Parity Flag (P) – If after any arithmetic or logical operation the result has even parity, an even number of 1 bits, the parity register becomes set i.e. 1, otherwise it becomes reset i.e. 0.
1-accumulator has even number of 1 bits
0-accumulator has odd parity</p>
<p>Example:</p>
<p>MVI A 05 (load 05H in register A)
This instruction will set the parity flag to 1 as the BCD code of 05H is 00000101, which contains even number of ones i.e. 2.</p>
<p>Carry Flag (CY) – Carry is generated when performing n bit operations and the result is more than n bits, then this flag becomes set i.e. 1, otherwise it becomes reset i.e. 0.
During subtraction (A-B), if A&gt;B it becomes reset and if (A&lt;B) it becomes set.
Carry flag is also called borrow flag.
1-carry out from MSB bit on addition or borrow into MSB bit on subtraction
0-no carry out or borrow into MSB bit</p>
<p>Example:</p>
<p>MVI A 30 (load 30H in register A)
MVI B 40 (load 40H in register B)
SUB B (A = A – B)
These set of instructions will set the carry flag to 1 as 30 – 40 generates a carry/borrow.</p>
<p>MVI A 40 (load 40H in register A)
MVI B 30 (load 30H in register B)
SUB B (A = A – B)
These set of instructions will reset the sign flag to 0 as 40 – 30 does not generate any carry/borrow.</p>
<p>Overflow Flag (O) – This flag will be set (1) if the result of a signed operation is too large to fit in the number of bits available to represent it, otherwise reset (0). After any operation, if D[6] generates any carry and passes to D[7] OR if D[6] does not generates carry but D[7] generates, overflow flag becomes set, i.e., 1. If D[6] and D[7] both generate carry or both do not generate any carry, then overflow flag becomes reset, i.e., 0.
Example: On adding bytes 100 + 50 (result is not in range -128…127), so overflow flag will set.</p>
<p>MOV AL, 50 (50 is 01010000 which is positive)
MOV BL, 32 (32 is 00110010 which is positive)
ADD AL, BL (82 is 10000010 which is negative)
Overflow flag became set as we added 2 +ve numbers and we got a -ve number.</p>
<p>(b) Control Flags – The control flags enable or disable certain operations of the microprocessor. There are 3 control flags in 8086 microprocessor and these are:</p>
<p>Directional Flag (D) – This flag is specifically used in string instructions.
If directional flag is set (1), then access the string data from higher memory location towards lower memory location.
If directional flag is reset (0), then access the string data from lower memory location towards higher memory location.
Interrupt Flag (I) – This flag is for interrupts.
If interrupt flag is set (1), the microprocessor will recognize interrupt requests from the peripherals.
If interrupt flag is reset (0), the microprocessor will not recognize any interrupt requests and will ignore them.
Trap Flag (T) – This flag is used for on-chip debugging. Setting trap flag puts the microprocessor into single step mode for debugging. In single stepping, the microprocessor executes a instruction and enters into single step ISR.
If trap flag is set (1), the CPU automatically generates an internal interrupt after each instruction, allowing a program to be inspected as it executes instruction by instruction.
If trap flag is reset (0), no function is performed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-segmentation-in-8086"><a class="header" href="#memory-segmentation-in-8086">Memory Segmentation in 8086</a></h1>
<p><img src="microprocessors/./images/8086_memory-segmentation.png" alt="memory-segmentation" /> </p>
<h2 id="concept-of-segmentation"><a class="header" href="#concept-of-segmentation">Concept of Segmentation</a></h2>
<ul>
<li>
<p>Segmentation means dividing the memory into logically different parts called segments.</p>
</li>
<li>
<p>8086 has a 20-bit address bus, hence it can access 2^20 Bytes i.e. 1MB memory.</p>
</li>
<li>
<p>But this also means that Physical address will now be 20 bit. It is not possible to work with a 20 bit address as it is not a byte compatible number. (20 bits is two and a half bytes). To avoid working with this incompatible number, we create a <strong>virtual model</strong> of the memory.</p>
</li>
<li>
<p>Here the memory is divided into 4 segments: Code, Stack Data and Extra.</p>
</li>
<li>
<p>The max size of a segment is 64KB and the minimum size is 16 bytes.</p>
</li>
<li>
<p>Now programmer can access each location with a VIRTUAL ADDRESS.</p>
</li>
<li>
<p>The Virtual Address is a combination of Segment Address and Offset Address.</p>
</li>
<li>
<p>Segment Address indicates where the segment is located in the memory (base address)</p>
</li>
<li>
<p>Offset Address gives the offset of the target location within the segment.</p>
</li>
<li>
<p>Since both, Segment Address and Offset Address are 16 bits each, they both are compatible numbers and can be easily used by the programmer.</p>
</li>
<li>
<p>Moreover, Segment Address is given only in the beginning of the program, to initialize the segment. Thereafter, we only give offset address.</p>
</li>
<li>
<p>Hence we can access 1 MB memory using only a 16 bit offset address for most part of the program. This is the advantage of segmentation.</p>
</li>
<li>
<p>Moreover, dividing Code, stack and Data into different segments, makes the memory more organized and prevents accidental overwrites between them.</p>
</li>
<li>
<p>The Maximum Size of a segment is 64KB because offset addresses are of 16 bits. 216 = 64KB.</p>
</li>
<li>
<p>As max size of a segment is 64KB, programmer can create multiple Code/Stack/Data segments till the entire 1 MB is utilized, but only one of each type will be currently active.</p>
</li>
<li>
<p>The physical address is calculated by the microprocessor, using the formula:</p>
<pre><code>PHYSICAL ADDRESS = SEGMENT ADDRESS X 10H + OFFSET ADDRESS
Ex: if Segment Address = 1234H and Offset Address is 0005H then
Physical Address = 1234H x 10H + 0005H = 12345H
</code></pre>
</li>
<li>
<p>This formula automatically ensures that the minimum size of a segment is 10H bytes
(10H = 16 Bytes).</p>
</li>
</ul>
<h2 id="code-segment"><a class="header" href="#code-segment">Code Segment</a></h2>
<ul>
<li>This segment is used to hold the <strong>program</strong> to be executed.</li>
<li><strong>Instruction are fetched</strong> from the Code Segment.</li>
<li>CS register holds the 16-bit base address for this segment.</li>
<li>IP register (Instruction Pointer) holds the 16-bit offset address.</li>
</ul>
<h2 id="data-segment"><a class="header" href="#data-segment">Data Segment</a></h2>
<ul>
<li>This segment is used to hold <strong>general data</strong>.</li>
<li>This segment also holds the <strong>source operands</strong> during <strong>string operations</strong>.</li>
<li>DS register holds the 16-bit base address for this segment.</li>
<li>BX register is used to hold the 16-bit offset for this segment.</li>
<li>SI register (Source Index) holds the 16-bit offset address during String Operations.</li>
</ul>
<h2 id="stack-segment"><a class="header" href="#stack-segment">Stack Segment</a></h2>
<ul>
<li>This segment holds the <strong>Stack memory</strong>, which operates in LIFO manner.</li>
<li>SS holds its Base address.</li>
<li>SP (Stack Pointer) holds the 16-bit offset address of the Top of the Stack.</li>
<li>BP (Base Pointer) holds the 16-bit offset address during <strong>Random Access</strong>.</li>
</ul>
<h2 id="extra-segment"><a class="header" href="#extra-segment">Extra Segment</a></h2>
<ul>
<li>This segment is used to hold <strong>general data</strong>.</li>
<li>Additionally, this segment is used as the destination during String Operations.</li>
<li>ES holds the Base Address.</li>
<li>DI holds the offset address during string operations.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="learning-objective"><a class="header" href="#learning-objective">Learning Objective</a></h2>
<ul>
<li>Learn and appreciate computer architecture with an emphasis on system design,
performance and analysis.</li>
<li>Elevate thinking process to the level of performance improvement techniques
for recent multi-core architectures.</li>
<li>Understand and analyze events happening at hardware level with the help of
open source simulators.</li>
<li>Enable exploration of future directions in computer architecture research.</li>
</ul>
<h2 id="focus"><a class="header" href="#focus">Focus</a></h2>
<ul>
<li>Part A: Processor design trends - instruction pipeline concepts, pipeline
hazards, out-of-order execution, static and dynamic scheduling, advanced
branch prediction techniques, multiple issue superscalar processors, vector
and GPU architectures.</li>
<li>Part B: Cache memory concepts and optimization techniques, DRAM organization,
memory controllers. Many-core processors; principles, design concepts and
microarchitecture of NoC.</li>
<li>Part C: Exploration in system design and analysis with the help of open
source architecture simulator GEM5</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="abstraction-of-modern-computer-architecture"><a class="header" href="#abstraction-of-modern-computer-architecture">Abstraction of Modern Computer Architecture</a></h2>
<ul>
<li>Application</li>
<li>Algorithm</li>
<li>Programming Language</li>
<li>Operating System/Virtual Machines</li>
<li><strong>Instruction Set Architecture</strong></li>
<li><strong>Microarchitectwre</strong></li>
<li><strong>Register-Transfer Level</strong></li>
<li>Gates</li>
<li>Circuits</li>
<li>Devices</li>
<li>Physics</li>
</ul>
<ul>
<li>
<p>ISA vs Microarchitecture</p>
</li>
<li>
<p>ISA Characteristics</p>
</li>
</ul>
<ul>
<li>Machine Models</li>
<li>Encoding</li>
<li>Data Types</li>
<li>Instructions</li>
<li>Addressing Modes</li>
</ul>
<p>For each operation that is to be carried out with respect to an instruction, we
have multiple sub operations like instruction fetch, decode, operand fetch,
result store and next instruction.</p>
<h2 id="instruction-execution-cycle"><a class="header" href="#instruction-execution-cycle">Instruction Execution Cycle</a></h2>
<ul>
<li>Instruction Fetch - Obtain instruction from program storage</li>
<li>Instruction Decode - Determine required actions and instruction size</li>
<li>Operand Fetch - Locate and obtain operand data</li>
<li>Execute - Compute result value or status</li>
<li>Result Store - Deposit results in storage for later use</li>
<li>Next Instruction - Determine successor instruction</li>
</ul>
<h2 id="processor-memory-interaction"><a class="header" href="#processor-memory-interaction">Processor Memory Interaction</a></h2>
<p>If the processor wants to fetch an instruction, the first thing that you have
to do is the address of the instruction which is available in the program
counter has to be transferred to a MAR. Similarly, if processor wanted to read
or write any data into memory, then also the address has to be kept inside MAR.
So MAR is a register which is known as memory address register, which contains
the address of the next word that has to be accessed in the memory.</p>
<p>It can be either for a read or a fetch operation or it can be for a right
operation. Now whatever is the address that is placed in MAR, the data contents
are being exchanged through MDR. So, if it is a write operation, then the
contents in MDR are transferred to memory on a location specified by MAR. If it
is for an instruction fetch or a read operation, then the contents of the
designated location specified by MAR are being transferred to the processor and
it reaches MDR first.</p>
<p><img src="computer-architecture/./images/inside-cpu.png" alt="Inside CPU" /> </p>
<h2 id="instruction-fetch"><a class="header" href="#instruction-fetch">Instruction FETCH</a></h2>
<ol>
<li>address of the next instruction is transferred from PC to MAR</li>
<li>the instruction is located in memory</li>
<li>instruction is copied from memory to MDR</li>
<li>instruction is transferred to and decoded in the IR</li>
<li>control unit sends signals to appropriate devices to cause execution of the instruction</li>
</ol>
<h2 id="memory-address-decoder"><a class="header" href="#memory-address-decoder">Memory Address Decoder</a></h2>
<p><img src="computer-architecture/./images/blck_mem.png" alt="Memory Block" /></p>
<h2 id="byte-ordering"><a class="header" href="#byte-ordering">Byte Ordering</a></h2>
<ul>
<li>Little Endian</li>
<li>Big Endian
<img src="computer-architecture/./images/byte-ordering.png" alt="Byte Ordering" /> </li>
</ul>
<h2 id="byte-alignment"><a class="header" href="#byte-alignment">Byte Alignment</a></h2>
<h3 id="data-alignment"><a class="header" href="#data-alignment">Data alignment</a></h3>
<p>Data alignment means putting the data in memory at address equal to some
multiple of the word size. This increases the performance of system due to the
way the CPU handles memory. </p>
<h3 id="data-structure-padding-now-to-align-the-data-it-may-be-necessary-to"><a class="header" href="#data-structure-padding-now-to-align-the-data-it-may-be-necessary-to">Data Structure Padding Now, to align the data, it may be necessary to</a></h3>
<p>insert some extra bytes between the end of the last data structure and the
start of the next data structure as the data is placed in memory as multiples
of fixed word size. This insertion of extra bytes of memory to align the data
is called data structure padding.</p>
<h2 id="types-of-operations"><a class="header" href="#types-of-operations">Types of operations</a></h2>
<h3 id="arithmetic-and-logical-operations"><a class="header" href="#arithmetic-and-logical-operations">Arithmetic and Logical Operations</a></h3>
<ul>
<li>integer arithmetic</li>
<li>comparing two quantities</li>
<li>shifting, rotating bits in a quantity</li>
<li>testing, comparing, and converting bits</li>
</ul>
<h3 id="data-movement-operations"><a class="header" href="#data-movement-operations">Data Movement Operations</a></h3>
<ul>
<li>moving data from memory to the CPU</li>
<li>moving data from memory to memory</li>
<li>input and output</li>
</ul>
<h3 id="program-control-operations"><a class="header" href="#program-control-operations">Program Control Operations</a></h3>
<ul>
<li>starting a program</li>
<li>halting a program</li>
<li>skipping to other instructions</li>
<li>testing data to decide whether to skip over some instructions</li>
</ul>
<h2 id="instruction-set-architecture-multiple-instructions-combined-together-to-form"><a class="header" href="#instruction-set-architecture-multiple-instructions-combined-together-to-form">Instruction Set Architecture Multiple instructions combined together to form</a></h2>
<p>program and multiple programs combined together to form software which will
basically is a big task. </p>
<p>Opcode specifies what needs to be done operand specifies where it needs to be
done. And the set of all possible instruction that a processor can do it is
known as instruction set architecture.</p>
<h2 id="classification-of-isa"><a class="header" href="#classification-of-isa">Classification of ISA</a></h2>
<h3 id="stack"><a class="header" href="#stack">Stack</a></h3>
<ul>
<li>Accumulator</li>
<li>Register-memory</li>
<li>Register-register / load-store</li>
</ul>
<h3 id="addressing-modes"><a class="header" href="#addressing-modes">Addressing Modes</a></h3>
<ul>
<li>Register            add r1, r2          r1 &lt;- rl+r2</li>
<li>Immediate           add r1, #5          r1 &lt;- r1+5</li>
<li>Direct              add r1, (0x200)     r1 &lt;- r1+M[0x200]</li>
<li>Register indirect   add r1, (r2)        r1 &lt;- r1+M[r2]</li>
<li>Displacement        add r1, 100(r2)     r1 &lt;- r1+M[r2+100]</li>
<li>Indexed             add r1, (r2+r3)     r1 &lt;- r1+M[r2+r3])</li>
<li>Scaled              add r1, (r2+r3<em>4)   r1 &lt;- r1+M[(r2+r3</em>4]</li>
<li>Memory indirect     add r1, @(r2)       r1 &lt;- r1+M[M[r2]]</li>
<li>Auto-increment      add r1, (r2)+       r1 &lt;- r1+M[r2], r2++</li>
<li>Auto-decrement      add r1, -(r2)       r2--, r1 &lt;- r1+M[r2]</li>
</ul>
<h2 id="architecture-vs-microarchitecture"><a class="header" href="#architecture-vs-microarchitecture">Architecture vs. Microarchitecture</a></h2>
<h3 id="architectureinstruction-set-architecture"><a class="header" href="#architectureinstruction-set-architecture">“Architecture”/Instruction Set Architecture:</a></h3>
<ul>
<li>Programmer visible state (Memory &amp; Register)</li>
<li>Operations (Instructions and how they work)</li>
<li>Execution Semantics (interrupts)</li>
<li>Input/Output</li>
<li>Data Types/Sizes</li>
</ul>
<h3 id="microarchitectureorganization"><a class="header" href="#microarchitectureorganization">Microarchitecture/Organization:</a></h3>
<ul>
<li>Tradeoffs on how to implement ISA for some metric (Speed, Energy, Cost)</li>
<li>Examples: Pipeline depth, number of pipelines, cache size, silicon area, peak
power, execution ordering, bus widths, ALU widths</li>
</ul>
<h2 id="why-the-diversity-in-isas-technology-influenced-isa"><a class="header" href="#why-the-diversity-in-isas-technology-influenced-isa">Why the Diversity in ISAs? Technology Influenced ISA</a></h2>
<ul>
<li>Storage is expensive, tight encoding important</li>
<li>Reduced Instruction Set Computer
<ul>
<li>Remove instructions until whole computer fits on die</li>
</ul>
</li>
<li>Multicore/Manycore
<ul>
<li>Transistors not turning into sequential performance</li>
</ul>
</li>
</ul>
<p>Application Influenced ISA</p>
<ul>
<li>Instructions for Applications
<ul>
<li>DSP instructions</li>
</ul>
</li>
<li>Compiler Technology has improved
<ul>
<li>SPARC Register Windows no longer needed</li>
<li>Compiler can register allocate effectively</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="when-can-we-say-one-computer--architecture--design-is-better-than-others"><a class="header" href="#when-can-we-say-one-computer--architecture--design-is-better-than-others">When can we say one computer / architecture / design is better than others?</a></h2>
<ul>
<li>Desktop PC (execution time of a program)</li>
<li>Server (transactions / unit time)</li>
</ul>
<h2 id="when-can-we-say-x-is-n-times-fasterthany-"><a class="header" href="#when-can-we-say-x-is-n-times-fasterthany-">When can we say X is n times fasterthanY ?</a></h2>
<ul>
<li>Execution time(y) / Execution time(x) =n</li>
<li>Throughput(x) / Throughput(y) = n</li>
</ul>
<h2 id="typical-performance-metrics"><a class="header" href="#typical-performance-metrics">Typical performance metrics</a></h2>
<ul>
<li>Response time - When a request is coming from a machine to another machine,
the time for the second machine to respond to that request is known as
response time</li>
<li>Throughput - Number of task that is completed in a unit time</li>
<li>CPU time - Total time associated with respect to a program in cpu execution</li>
<li>Wall clock time - Actual time taken for a program to execute ie sum total of
cpu time and non-cpu time(peripheral time)</li>
<li>Speedup - If the execution time of one is larger than other then it means
speedup</li>
</ul>
<h2 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h2>
<ul>
<li>Toy programs (e.g. sorting, matrix multiply)</li>
<li>Synthetic benchmarks (e.g. Dhrystone)</li>
<li>Benchmark suites (e.g. SPEC06, SPLASH)</li>
</ul>
<h2 id="specratio"><a class="header" href="#specratio">SPECRatio</a></h2>
<p>SPECRatio(A) = Execution time(reference) / Execution time(A)</p>
<p>Reference for SPEC 2006:
Sun Ultra Enterprise 2 workstation with a 296-MHz UltraSPARC II processor</p>
<h2 id="amdhals-law"><a class="header" href="#amdhals-law">Amdhal's Law</a></h2>
<ul>
<li>Amdahl’s Law defines the speedup that can be gained by improving some portion
of a computer.</li>
<li>The performance improvement to be gained from using some faster mode of
execution is limited by the fraction of the time the faster mode can be used.</li>
</ul>
<p><img src="computer-architecture/./images/amdahls-law.png" alt="Amdahl's Law" /></p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>Suppose that we want to enhance the floating point operations of a processor by
introducing a new advanced FPU unit. Let the new FPU is 10 times faster on
floating point computations than the original processor. Assuming a program has
40% floating point operations, what is the overall speedup gained by
incorporating the enhancement?</p>
<p>Solution:</p>
<ul>
<li>Fraction enhanced = 0.4</li>
<li>Speedup enhanced = 10</li>
<li>Therefore speedup = 1/(0.6 + 0.4/10) = 1.56 times faster</li>
</ul>
<h2 id="example-1"><a class="header" href="#example-1">Example</a></h2>
<p>A common transformation required in graphics processors is square root.
Implementations of floating-point (FP) square root vary significantly in
performance, especially among processors designed for graphics. Suppose FP
square root (FPSQR) is responsible for 20% of the execution time of a critical
graphics benchmark.</p>
<p>One proposal is to enhance the FPSQR hardware and speed up this operation by a
factor of 10. The other alternative is just to try to make all FP instructions
in the graphics processor run faster by a factor of 1.6; FP instructions are
responsible for half of the execution time for the application. Compare these
two design alternatives using Amdahl's Law.</p>
<p>Solution:</p>
<ul>
<li>Case A: FPSQR hardware optimization 
<ul>
<li>S = 1.219</li>
</ul>
</li>
<li>Case B: FP instructions optimization
<ul>
<li>S = 1.23</li>
</ul>
</li>
</ul>
<h2 id="principles-of-computer-design"><a class="header" href="#principles-of-computer-design">Principles of Computer Design</a></h2>
<ul>
<li>All processors are driven by clock.</li>
<li>Expressed as clock rate in GHz or clock period in ns</li>
<li>CPU Time = CPU clock cycles x clock cycle time</li>
</ul>
<p>CPl = CPU clock cycles for a program / Instruction Count
CPU Time = IC * CPI * CCT</p>
<ul>
<li>Clock cycle time- hardware technology</li>
<li>CPI- Organization and ISA</li>
<li>IC-ISA and compiler technology</li>
</ul>
<h3 id="example-basic-performance-analysis"><a class="header" href="#example-basic-performance-analysis">Example: Basic Performance Analysis</a></h3>
<p>Consider two programs A and B that solves a given problem. A is scheduled to
run on a processor P1 operating at 1 GHz and B is scheduled to run on processor
P2 running at 1.4 GHz. A has total 10000 instructions, out of which 20% are
branch instructions, 40% load store instructions and rest are ALU instructions.
B is composed of 25% branch instructions. The number of load store instructions
in B is twice the count of ALU instructions. Total instruction count of B is
12000. In both P1 and P2 branch instructions have an average CPI of 5 and ALU
instructions has an average CPI of 1.5. Both the architectures differ in the
CPI of load-store instruction. They are 2 and 3 for P1 and P2, respectively.
Which mapping (A on P1 or B on P2) solves the problem faster, and by how much?</p>
<p><img src="computer-architecture/./images/example-performance-analysis.png" alt="Performance Analysis Solution" /> </p>
<h3 id="amdahls-law"><a class="header" href="#amdahls-law">Amdahl's Law</a></h3>
<p>A company is releasing 2 latest versions (beta and gamma) of its basic
processor architecture named alpha. Beta and gamma are designed by
making modifications on three major components (X, Y and Z) of the alpha.
It was observed that for a program A the fractions of the total execution time
on these three components, X, Y, and Z are 40%, 30%, and 20%,
respectively. Beta speeds up X and Z by 2 times but slows down Y by 1.3
times, where as gamma speeds up X, Y and Z by 1.2, 1.3 and 1.4 times,
respectively.
a. How much faster is gamma over alpha for running A?
b. Whether beta or gamma is faster for running A? Find the speedup factor</p>
<p>Solution:
a. Gamma is 1.239 times faster over alpha
b. Beta is faster than gamma -&gt; 1.267/1.239 = 1.022 times</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-mips"><a class="header" href="#introduction-to-mips">Introduction to MIPS</a></h1>
<ul>
<li>Microprocessor without Interlocked Pipelined Stages</li>
<li>32 registers (32 bit each)</li>
<li>Uniform length instructions</li>
<li>RISC- Load store architecture</li>
</ul>
<h2 id="advantages-of-mips"><a class="header" href="#advantages-of-mips">Advantages of MIPS</a></h2>
<ul>
<li>It is easy to understand and measure</li>
<li>It helps in calculation of CPU processor speed (cycles per second), CPI
(average clock cycles per instruction) and Execution time.</li>
<li>It handles when amount of work is large.</li>
</ul>
<h2 id="disadvantages-of-mips"><a class="header" href="#disadvantages-of-mips">Disadvantages of MIPS</a></h2>
<ul>
<li>It may not reflect real execution, since simple instructions do way better.</li>
<li>It is an older, obsolete measure of a computer’s speed and power</li>
</ul>
<h2 id="instruction-representation"><a class="header" href="#instruction-representation">Instruction Representation</a></h2>
<p><img src="computer-architecture/./images/mips-instruction-representation.png" alt="Instruction Representation" /></p>
<ul>
<li>op: basic operation of the instruction (opcode)</li>
<li>rs: first source operand register</li>
<li>rt: second source operand register</li>
<li>rd: destination operand register</li>
<li>shamt: shift amount</li>
<li>funct: selects the specific variant of the opcode (function code)</li>
<li>address: offset for load/store instructions (+/-2^15)</li>
<li>immediate: constants for immediate instructions</li>
</ul>
<p>R = Register type instruction</p>
<h2 id="pipeline-characteristics"><a class="header" href="#pipeline-characteristics">Pipeline Characteristics</a></h2>
<ul>
<li>Pipelining doesn't reduce latency of single task, it improves throughput of
entire workload</li>
<li>Pipeline rate limited by slowest pipeline stage</li>
<li>Potential speedup = Number of pipe stages</li>
<li>Unbalanced lengths of pipe stages reduces speedup</li>
<li>Time to fill pipeline and time to drain it reduces speedup</li>
</ul>
<h2 id="pipeline-in-circuits"><a class="header" href="#pipeline-in-circuits">Pipeline in Circuits</a></h2>
<ul>
<li>Pipelining partitions the system into multiple independent stages with added
buffers between the stages.</li>
<li>Pipelining can increase the throughout of a system.</li>
</ul>
<p><img src="computer-architecture/./images/pipeline-in-circuits.png" alt="Pipeline" /></p>
<p>Dividing the actual circuit which was n-logic gates into smaller sub components
and to interface with them with latches is called pipelining inside the
circuits</p>
<h2 id="pipeline-in-mips"><a class="header" href="#pipeline-in-mips">Pipeline in MIPS</a></h2>
<p><img src="computer-architecture/./images/mips-pipeline.png" alt="MIPS Pipeline" /></p>
<ul>
<li>Each instruction can take at most 5 clock cycles</li>
</ul>
<ol>
<li>Instruction fetch cycle (IF)</li>
</ol>
<ul>
<li>Based on PC, fetch the instruction from memory</li>
<li>Increment PC</li>
</ul>
<ol start="2">
<li>Instruction decode/register fetch cycle (ID)</li>
</ol>
<ul>
<li>Decode the instruction + register read operation</li>
<li>Fixed field decoding</li>
<li>Ex: [ADD R1,R2,R3] : A3.01.02.03
<ul>
<li>10100011 00000001 00000010 00000011</li>
</ul>
</li>
<li>Ex: [LW R1,8(R2)] : 86.01.02.03
<ul>
<li>10000110 00000001 00001000 00000010</li>
</ul>
</li>
<li>Equality check of registers</li>
<li>Computation of branch target address if any</li>
</ul>
<ol start="3">
<li>Execution/Effective address cycle (EX)</li>
</ol>
<ul>
<li>Memory reference: Calculate the effective address
<ul>
<li>[LW R1,8(R2)]       Effective ADDR= [R2] +8</li>
</ul>
</li>
<li>Register-register ALU instruction
<ul>
<li>[ADD R1,R2,R2]      Actual execution ie R2+R3</li>
</ul>
</li>
<li>Register-immediate ALU instruction</li>
</ul>
<ol start="4">
<li>Memory access cycle (MEM)</li>
</ol>
<ul>
<li>Load instruction: Read from memory using effective address [LW R1,8(R2)]</li>
<li>Store instruction: Write the data in the register to memory using effective
address [SW R3,16(R4)]</li>
</ul>
<ol start="5">
<li>Write-back cycle (WB)</li>
</ol>
<ul>
<li>
<p>Register-register ALU instruction or load instruction</p>
</li>
<li>
<p>Write the result to register file [LW R1,8(R2)], [ADD R1,R2,R3]</p>
</li>
<li>
<p>Cycles required to implement different instructions</p>
<ul>
<li>Branch instructions — 4 cycles</li>
<li>Store instructions — 4 cycles</li>
<li>All other instructions — 5 cycles</li>
</ul>
</li>
</ul>
<h2 id="pipeline-issues"><a class="header" href="#pipeline-issues">Pipeline Issues</a></h2>
<ul>
<li>
<p>Ideal Case: Uniform sub-computations</p>
<ul>
<li>The computation to be performed can be evenly partitioned into uniform-latency sub-computations</li>
</ul>
</li>
<li>
<p>Reality: Internal fragmentation</p>
<ul>
<li>Not all pipeline stages may have the uniform latencies</li>
</ul>
</li>
<li>
<p>Impact of ISA</p>
<ul>
<li>Memory access is a critical sub-computation</li>
<li>Memory addressing modes should be minimized</li>
<li>Fast cache memories should be employed</li>
</ul>
</li>
<li>
<p>Ideal Case : Identical computations</p>
<ul>
<li>The same computation is to be performed repeatedly on a large number of input data sets</li>
</ul>
</li>
<li>
<p>Reality: External fragmentation</p>
<ul>
<li>Some pipeline stages may not be used</li>
</ul>
</li>
<li>
<p>Impact of ISA</p>
<ul>
<li>Reduce the complexity and diversity of the instruction types</li>
<li>RISC architectures use uniform stage simple instructions</li>
</ul>
</li>
<li>
<p>Ideal Case : Independent computations</p>
<ul>
<li>All the instructions are mutually independent</li>
</ul>
</li>
<li>
<p>Reality: Pipeline stalls - cannot proceed</p>
<ul>
<li>A later computation may require the result of an earlier computation</li>
</ul>
</li>
<li>
<p>Impact of ISA</p>
<ul>
<li>Reduce Memory addressing modes - dependency detection</li>
<li>Use register addressing mode - easy dependencies check</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pipeline-hazards"><a class="header" href="#pipeline-hazards">Pipeline Hazards</a></h1>
<ul>
<li>Hazards: circumstances that would cause incorrect execution if next
instruction is fetched and executed</li>
<li><strong>Structural hazards:</strong> Different instructions, at different stages, in the
pipeline want to use the same hardware resource</li>
<li><strong>Data hazards:</strong> An instruction in the pipeline requires data to be computed by
a previous instruction still in the pipeline</li>
<li><strong>Control hazards:</strong> Succeeding instruction, to put into pipeline, depends on the
outcome of a previous branch instruction, already in pipeline</li>
</ul>
<h2 id="structural-hazard"><a class="header" href="#structural-hazard">Structural Hazard</a></h2>
<ul>
<li>
<p>Eliminate the use same hardware for two different things at the same time</p>
</li>
<li>
<p>Solution 1: Wait</p>
<ul>
<li>must detect the hazard</li>
<li>must have mechanism to stall</li>
</ul>
</li>
<li>
<p>Solution 2: Duplicate hardware</p>
<ul>
<li>Multiple such units will help both instruction to progress
<img src="computer-architecture/./images/eliminating-structural-hazard.png" alt="Eliminating Structural Hazard" /> </li>
</ul>
</li>
</ul>
<h2 id="data-hazard"><a class="header" href="#data-hazard">Data Hazard</a></h2>
<h3 id="read-after-writeraw"><a class="header" href="#read-after-writeraw">Read After Write(RAW)</a></h3>
<ul>
<li>Instr-2 tries to read operand before Instr-1, writes it
<pre><code>Instr-1: add r1,r2,r3
Instr-2: sub r4,r1,r3
</code></pre>
</li>
</ul>
<h3 id="write-after-read-war"><a class="header" href="#write-after-read-war">Write After Read (WAR)</a></h3>
<ul>
<li>Instr-2, writes operand before Instr-1, reads it
<pre><code>Instr-1: sub r4,r1,r3
Instr-2: add r1,r2,r3
Instr-3: mul r6,r1,r7
</code></pre>
</li>
<li>Called an anti-dependence by compiler writers.</li>
<li>This results from reuse of the name r1</li>
<li>Can’t happen in MIPS 5 stage pipeline because:
<ul>
<li>All instructions take 5 stages, and</li>
<li>Reads are always in stage 2, and</li>
<li>Writes are always in stage 5</li>
</ul>
</li>
</ul>
<h3 id="write-after-write-waw"><a class="header" href="#write-after-write-waw">Write After Write (WAW)</a></h3>
<ul>
<li>Instr-2, writes operand before Instr-1, writes it.
<pre><code>Instr-1: sub r1,r4,r3
Instr-2: add r1,r2,r3
Instr-3: mul r6,r1,r7
</code></pre>
</li>
<li>Called an output dependence</li>
<li>This also results from the reuse of name r1</li>
<li>Can’t happen in MIPS 5 stage pipeline because:
<ul>
<li>All instructions take 5 stages, and</li>
<li>Writes are always in stage 5</li>
</ul>
</li>
<li>WAR and WAW happens in out of order pipes</li>
</ul>
<h3 id="handle-data-hazards"><a class="header" href="#handle-data-hazards">Handle Data hazards</a></h3>
<ul>
<li>Data hazard: instruction needs data from the result of a previous
instruction still executing in pipeline</li>
<li>Solution: Forward data if possible</li>
</ul>
<p><img src="computer-architecture/./images/data-forwarding.png" alt="Data Forwarding" /> </p>
<ul>
<li>Load ALU Hazard</li>
<li>Solution: Delay the next instruction (add bubble)</li>
<li>Software Solution: Arrange the instructions while compiling to avoid hazard</li>
</ul>
<h2 id="control-hazard"><a class="header" href="#control-hazard">Control Hazard</a></h2>
<ul>
<li>Normal MIPS Pipeline
<img src="computer-architecture/./images/conventional-mips-pipeline.png" alt="Normal" /> </li>
<li>Modern MIPS Pipeline
<img src="computer-architecture/./images/modern-mips-pipeline.png" alt="Modern" /> </li>
</ul>
<h3 id="four-branch-hazard-alternatives"><a class="header" href="#four-branch-hazard-alternatives">Four Branch Hazard Alternatives</a></h3>
<h4 id="1-stall-until-branch-direction-is-clear"><a class="header" href="#1-stall-until-branch-direction-is-clear">1: Stall until branch direction is clear</a></h4>
<h4 id="2-predict-branch-not-taken"><a class="header" href="#2-predict-branch-not-taken">2: Predict Branch Not Taken</a></h4>
<ul>
<li>Execute successor instructions in sequence</li>
<li>&quot;Squash&quot; instructions in pipeline if branch actually taken</li>
</ul>
<h4 id="3-predict-branch-taken"><a class="header" href="#3-predict-branch-taken">3: Predict Branch Taken</a></h4>
<ul>
<li>But branch target address in is not known by IF stage</li>
<li>Target is known at same time as branch outcome (IDstage)</li>
<li>MIPS still incurs 1 cycle branch penalty</li>
</ul>
<h4 id="4-delayed-branch"><a class="header" href="#4-delayed-branch">4: Delayed Branch</a></h4>
<ul>
<li>Define branch to take place AFTER one instruction following the branch
instruction</li>
<li>1 slot delay allows proper decision and branch target address in 5 stage
pipeline (MIPS uses this approach)</li>
<li>Where to get instructions to fill branch delay slot?
<img src="computer-architecture/./images/filling-branch-delay-slot.png" alt="bds" /> </li>
</ul>
<h2 id="conditional-branches"><a class="header" href="#conditional-branches">Conditional Branches</a></h2>
<ul>
<li>When do you know you have a branch?
<ul>
<li>During ID cycle (Could you know before that?)</li>
</ul>
</li>
<li>When do you know if the branch is Taken or Not-Taken
<ul>
<li>During EXE cycle/ ID stage depending on the design</li>
</ul>
</li>
<li>We need for sophisticated solutions for following cases
<ul>
<li>Modern pipelines are deep ( 10 + stages)</li>
<li>Several instructions issued/cycle</li>
<li>Several predicted branches in-flight at the same time</li>
</ul>
</li>
</ul>
<h2 id="dynamic-branch-prediction"><a class="header" href="#dynamic-branch-prediction">Dynamic branch prediction</a></h2>
<ul>
<li>
<p>Execution of a branch requires knowledge of:</p>
</li>
<li>
<p>Branch instruction - encode whether instruction is a branch or not. Decide on
taken or not taken (i.e., prediction can be done at IF stage)</p>
</li>
<li>
<p>Whether the branch is Taken/Not-Taken (hence a branch prediction mechanism)</p>
</li>
<li>
<p>If the branch is taken what is the target address (can be computed but can
also be &quot;precomputed&quot;, i.e., stored in some table)</p>
</li>
<li>
<p>If the branch is taken what is the instruction at the branch target address
(saves the fetch cycle for that instruction)</p>
</li>
<li>
<p>Use a <strong>Branch Prediction Buffer(BPB)</strong></p>
<ul>
<li>Also called Branch Prediction Table (BPT), Branch History Table (BHT)</li>
<li>Records previous outcomes of the branch instruction</li>
<li>How to index into the table is an issue</li>
</ul>
</li>
<li>
<p>A prediction using BPB is attempted when the branch instruction is fetched
(IF stage or equivalent)</p>
</li>
<li>
<p>It is acted upon during ID stage (when we know we have a branch)</p>
</li>
<li>
<p>Has a prediction been made (Y/N)</p>
<ul>
<li>If not use default &quot;Not Taken&quot;</li>
</ul>
</li>
<li>
<p>Is it correct or incorrect ?</p>
</li>
<li>
<p>Two cases:</p>
<ul>
<li>Case 1: Yes and the prediction was correct (known at ID stage) or No but
the default was correct: No delay</li>
<li>Case 2: Yes and the prediction was incorrect or No and the default was
incorrect: Delay</li>
</ul>
</li>
</ul>
<h3 id="prediction-scheme-with-1-or-2-bit-fsm"><a class="header" href="#prediction-scheme-with-1-or-2-bit-fsm">Prediction Scheme with 1 or 2 bit FSM</a></h3>
<p><img src="computer-architecture/./images/prediction-1-bit.png" alt="prediction-1-bit" />
<img src="computer-architecture/./images/prediction-2-bit.png" alt="prediction-2-bit" /> </p>
<ul>
<li>The use of a 2-bit predictor will allow branches that favor taken (or not
taken) to be mispredicted less often than the one-bit case. (reinforcement
learning)</li>
</ul>
<h3 id="branch-prediction-in-hardware"><a class="header" href="#branch-prediction-in-hardware">Branch Prediction In Hardware</a></h3>
<ul>
<li>Branch prediction is extremely useful in loops.</li>
<li>A simple branch prediction can be implemented using a small amount of memory
indexed by lower order bits of the address of the branch instruction. (branch
prediction buffer)</li>
<li>One bit stores whether the branch was taken or not</li>
<li>The next time the branch instruction is fetched refer this bit</li>
</ul>
<h3 id="advanced-branch-prediction-techniques"><a class="header" href="#advanced-branch-prediction-techniques">Advanced Branch Prediction Techniques</a></h3>
<h4 id="basic-2-bit-predictor"><a class="header" href="#basic-2-bit-predictor">Basic 2-bit predictor:</a></h4>
<ul>
<li>For each branch:- Predict T or NT</li>
<li>If the prediction is wrong for two consecutive times, change prediction</li>
</ul>
<h4 id="correlating-predictor"><a class="header" href="#correlating-predictor">Correlating predictor:</a></h4>
<ul>
<li>Multiple 2-bit predictors for each branch</li>
<li>One for each possible combination of outcomes of preceding n branches</li>
</ul>
<pre><code>if(x==2)  /*br-1*/
x=0;
if(y==2)  /*br-2*/
y=0;
if(x!=y)  /*br-3*/
    do this
else do that
</code></pre>
<h4 id="local-predictor"><a class="header" href="#local-predictor">Local predictor</a></h4>
<ul>
<li>Multiple 2-bit predictors for each branch</li>
<li>One for each possible combination of outcomes for the last n occurrences of this branch</li>
</ul>
<h4 id="tournament-predictor"><a class="header" href="#tournament-predictor">Tournament predictor</a></h4>
<ul>
<li>Combine correlating predictor with local predictor</li>
</ul>
<h2 id="branch-target-buffer"><a class="header" href="#branch-target-buffer">Branch-Target Buffer</a></h2>
<ul>
<li>To reduce the branch penalty, know whether the as-yet-un-decoded instruction is a branch. If so, what the next program counter (PC) should be</li>
<li>If the instruction is a branch and we know what the next PC should be, we can have a branch penalty of zero</li>
<li>A branch-prediction cache that stores the predicted address for the next instruction after a branch is called a branch-target buffer (BTB) or branch-target cache.
<img src="computer-architecture/./images/branch-target-buffer.png" alt="branch-target-buffer" /> 
<img src="computer-architecture/./images/branch-target-buffer-1.png" alt="branch-target-buffer-1" /> </li>
</ul>
<h2 id="branch-folding"><a class="header" href="#branch-folding">Branch Folding</a></h2>
<ul>
<li>Optimization on BTB to make zero cycle branch 
<ul>
<li>Larger branch-target buffer- store one or more target instructions</li>
<li>Add target instruction into BTB to deal with longer decoding time required by larger buffer</li>
<li>Branch folding can be used to obtain 0-cycle unconditional branches and sometimes 0-cycle conditional branches</li>
</ul>
</li>
</ul>
<h1 id="questions"><a class="header" href="#questions">Questions</a></h1>
<h2 id="example-1"><a class="header" href="#example-1">Example 1</a></h2>
<p>Given a non-pipelined architecture running at 1.5 GHz, that takes 5 cycles to
finish an instruction. You want to make it pipelined with 5 stages. Due to
hardware overhead the pipelined design will operate only at 1 GHz. 5% of memory
instructions cause a stall of 50 cycles, 30% of branch instruction cause a
stall of 2 cycles and load-ALU combinations cause a stall of 1 cycle. Assume
that in a given program, there exist 20% of branch instructions and 30% of
memory instructions. 10% of instructions are load-ALU combinations. What is the
speedup of pipelined design over the non-pipelined design?</p>
<p>Ans:
<img src="computer-architecture/./images/pipeline-hazards-solution1.png" alt="pipeline hazard soution 1" /> </p>
<h2 id="example-2"><a class="header" href="#example-2">Example 2</a></h2>
<p>A program has 2000 instructions in the sequence L.D, ADD.D, L.D, ADD.D,.....
L.D, ADD.D. The ADD.D instruction depends on the L.D instruction right before
it. The L.D instruction depends on the ADD.D instruction right before it. If
the program is executed on the 5-stage pipeline what would be the actual CPI
with and without operand forwarding technique?</p>
<p>Ans:
<img src="computer-architecture/./images/pipeline-hazards-solution2.1.png" alt="pipeline hazard soution 2" /> 
<img src="computer-architecture/./images/pipeline-hazards-solution2.2.png" alt="pipeline hazard soution 2" /> </p>
<h2 id="example-3-branch-prediction"><a class="header" href="#example-3-branch-prediction">Example 3: Branch Prediction</a></h2>
<p>Consider the last 16 actual outcomes of a single static branch. T means branch
is taken and N means not taken.</p>
<p>{oldest&gt; TTNNTNTTTNTNTTNT &lt; latest}</p>
<p>A two level branch predictor of (1,2) type is used. Since there is only one
branch in the program indexing to BHT with PC is irrelevant. Hence only last
branch outcome only is used to index to the BHT. How many mis-predictions are
there and which of the branches in this sequence would be mis-predicted? Fill
up the table for 16 branch outcomes.</p>
<p>Ans:
<img src="computer-architecture/./images/pipeline-hazards-solution3.png" alt="pipeline-hazards-solution3" /> </p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<ul>
<li>
<p>Pipelining overlaps execution of instructions
Exploits Instruction Level Parallelism (ILP)</p>
</li>
<li>
<p>There are two main approaches:</p>
<ul>
<li>Compiler-based static approaches</li>
<li>Hardware-based dynamic approaches</li>
</ul>
</li>
<li>
<p>Exploiting ILP is to minimize CPI</p>
<ul>
<li>Pipeline CPI = Ideal (base) CPI + Structural stalls + Data hazard stalls +
Control stalls</li>
</ul>
</li>
</ul>
<h2 id="parallelism-limitation-within-basic-block"><a class="header" href="#parallelism-limitation-within-basic-block">Parallelism limitation within Basic Block</a></h2>
<ul>
<li>The basic block- a straight-line code sequence without branches in except to
the entry and no branches out except at the exit.</li>
<li>Parallelism with basic block is limited. Typical size of basic block few
instructions only. Must optimize across multiple blocks (branches)</li>
</ul>
<p><img src="computer-architecture/./images/parallelism-limitation.png" alt="Parallelism limitation within Basic Block" /></p>
<h2 id="data-dependence"><a class="header" href="#data-dependence">Data Dependence</a></h2>
<ul>
<li>Loop-Level Parallelism
<ul>
<li>Unroll loop statically or dynamically</li>
</ul>
</li>
<li>Challenges—&gt; Data dependency</li>
<li>Data dependence conveys possibility of a hazard</li>
<li>Dependent instructions cannot be executed simultaneously</li>
<li>Pipeline determines if dependence is detected and if it causes a stall or not</li>
<li>Data dependence conveys upper bound on exploitable instruction level
parallelism</li>
</ul>
<h2 id="name-dependence--output-dependence"><a class="header" href="#name-dependence--output-dependence">Name Dependence &amp; Output dependence</a></h2>
<ul>
<li>Two instructions use the same name but no flow of information.</li>
<li>Not a true data dependence, but is a problem when reordering instructions</li>
<li><strong>Antidependence:</strong> instruction j writes a register or memory location that instruction i reads
<ul>
<li>Initial ordering (i before j) must be preserved</li>
</ul>
</li>
<li><strong>Output dependence:</strong> instruction i and instruction j write the same register or memory location
<ul>
<li>Ordering must be preserved</li>
</ul>
</li>
<li>To solve, use register renaming techniques</li>
</ul>
<h2 id="control-dependence"><a class="header" href="#control-dependence">Control Dependence</a></h2>
<ul>
<li>Ordering of instruction with respect to a branch instruction
<ul>
<li>Instruction that is control dependent on a branch cannot be moved <strong>before</strong>
the branch so that its execution is no longer controller by the branch</li>
<li>An instruction that is not control dependent on a branch cannot be moved
<strong>after</strong> the branch so that its execution is controlled by the branch.
<pre><code>if p1 {S1;};
if p2 {S2;};
</code></pre>
</li>
</ul>
</li>
<li>Instruction that is control dependent on a branch cannot be moved <strong>before</strong> the
branch so that its execution is no longer controller by the branch</li>
<li>An instruction that is not control dependent on a branch cannot be moved
<strong>after</strong> the branch so that its execution is controlled by the branch.
<img src="computer-architecture/./images/control-dep-examples.png" alt="control dep examples" /> </li>
</ul>
<h2 id="compiler-techniques-for-exposing-ilp"><a class="header" href="#compiler-techniques-for-exposing-ilp">Compiler Techniques for Exposing ILP</a></h2>
<ul>
<li>Find and overlap sequence of unrelated instruction </li>
<li>Pipeline scheduling
<ul>
<li>Separate dependent instruction from the source instruction by pipeline
latency of the source instruction</li>
</ul>
<pre><code>Example:
for (i=999; i&gt;=0; i=i-1)
  x[i] = X[i] +s;
</code></pre>
<img src="computer-architecture/./images/compiler-techniques-table.png" alt="compiler techniques table" /></li>
</ul>
<p>MIPS Assembly code for above code:</p>
<pre><code>loop:
  L.D F0,0(R1)
  stall
  ADD.D F4,F0,F2
  stall
  stall
  S.D F4,0(R1)
  DADDUI R1,R1,#-8
  stall (assume integer load latency is 1)
  BNE R1,R2,Loop
</code></pre>
<p>Scheduled Code:</p>
<pre><code>loop:
  L.D FO,0(R1)
  DADDUI R1,R1,#-8
  ADD.D F4,F0,F2
  stall
  stall
  S.D F4,8(R1)
  BNE R1.R2.Loop
</code></pre>
<h2 id="loop-unrolling"><a class="header" href="#loop-unrolling">Loop Unrolling</a></h2>
<p><img src="computer-architecture/./images/loop-unrollung.png" alt="loop unrollung" /></p>
<h2 id="loop-unrolling--pipeline-scheduling"><a class="header" href="#loop-unrolling--pipeline-scheduling">Loop Unrolling / Pipeline Scheduling</a></h2>
<p><img src="computer-architecture/./images/loop-unrollung-scheduling.png" alt="loop unrollung" /> </p>
<h2 id="strip-mining"><a class="header" href="#strip-mining">Strip Mining</a></h2>
<ul>
<li>Unknown number of loop iterations?
<ul>
<li>Goal: make k copies of the loop body Number of iterations = n</li>
<li>Generate pair of loops:
<ul>
<li>First executes n mod k times</li>
<li>Second executes n / k times</li>
<li>Strip mining</li>
</ul>
</li>
<li>Example: Let n=35, k=4
<ul>
<li>Loop 1 execute 3 times</li>
<li>Loop 2 execute 8 times by unrolling 4 copies per iteration</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="steps-in-loop-unrolling-and-scheduling"><a class="header" href="#steps-in-loop-unrolling-and-scheduling">Steps in Loop Unrolling and Scheduling</a></h2>
<ul>
<li>Determine that unrolling the loop would be useful.</li>
<li>Identify independency of loop iterations.</li>
<li>Use different registers to avoid unnecessary constraints put in on same
computations.</li>
<li>Eliminate the extra test and branch instructions and adjust the loop
termination and iteration code.</li>
<li>Determine whether the loads and stores from different iterations are
independent.</li>
<li>Schedule the code, preserving any dependences needed to yield the same result
as the original code.</li>
</ul>
<h2 id="loop-unrolling--pipeline-scheduling-1"><a class="header" href="#loop-unrolling--pipeline-scheduling-1">Loop Unrolling &amp; Pipeline Scheduling</a></h2>
<ul>
<li>Limitations of loop unrolling:
<ul>
<li>Code size limitations — I-cache miss</li>
<li>Compiler limitations — register pressure</li>
</ul>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
